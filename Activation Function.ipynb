{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4b2e26-941e-4717-8f67-b4a375d2a255",
   "metadata": {},
   "source": [
    "#### Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45f0e3-d8a5-464b-85d3-5b84839acae7",
   "metadata": {},
   "source": [
    " An activation function in the context of artificial neural networks is a mathematical operation applied to the output of each neuron in a neural network. It determines whether the neuron should be activated or not based on whether the neuron's input exceeds a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbd8dc-e006-4cdb-9184-7cd1c0886647",
   "metadata": {},
   "source": [
    "#### Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dba6c-31ce-4a14-a6a4-7055e122cff6",
   "metadata": {},
   "source": [
    "##### Some common types of activation functions used in neural networks include:\n",
    "\n",
    "1.Sigmoid function\n",
    "\n",
    "2.Hyperbolic tangent (tanh) function\n",
    "\n",
    "3.Rectified Linear Unit (ReLU)\n",
    "\n",
    "4.Leaky ReLU\n",
    "\n",
    "5.Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a692b-a3e7-45bf-a744-2d570f6a35d8",
   "metadata": {},
   "source": [
    "#### Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2c347-041d-4521-8372-1971a7e572c2",
   "metadata": {},
   "source": [
    "Activation functions affect the training process and performance of a neural network by introducing non-linearities into the model, enabling it to learn complex relationships in the data. The choice of activation function can impact the convergence speed during training, the model's ability to capture intricate patterns, and the prevention of issues like vanishing or exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13a1fe-6b43-482f-8b3e-47c13245d7d4",
   "metadata": {},
   "source": [
    "#### Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9ce2f-dc6d-4312-9abd-6f4e914a1645",
   "metadata": {},
   "source": [
    "The sigmoid activation function maps the input values to a range between 0 and 1. It's defined as \n",
    "ùúé\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ùëí\n",
    "‚àí\n",
    "ùë•\n",
    "œÉ(x)= \n",
    "1+e \n",
    "‚àíx\n",
    " \n",
    "1\n",
    "‚Äã\n",
    " . Its advantages include smoothness and easy interpretation as a probability, but it suffers from the vanishing gradient problem, where gradients become very small for large input values, slowing down the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90b328-254a-47b8-a9bf-9084ae5cd0bb",
   "metadata": {},
   "source": [
    "#### Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef9b25-d91a-4cbe-b42d-681cbda78cc9",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit (ReLU) activation function returns the input if it's positive and zero otherwise. Mathematically, it's defined as \n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "max\n",
    "‚Å°\n",
    "(\n",
    "0\n",
    ",\n",
    "ùë•\n",
    ")\n",
    "f(x)=max(0,x). Unlike the sigmoid function, ReLU is not bound to a specific output range and does not suffer from the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2c41a-9075-49bd-a442-514b0ec6cd31",
   "metadata": {},
   "source": [
    "#### Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0151ccc-cc08-4995-aacd-e8a389d7de0a",
   "metadata": {},
   "source": [
    "The benefits of using the ReLU activation function over the sigmoid function include faster convergence during training due to its non-saturating nature (avoids vanishing gradient), simpler computation, and reduced likelihood of the \"exploding gradient\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c99966-a3eb-44ff-9abe-d297fe199b1c",
   "metadata": {},
   "source": [
    "#### Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e47f3d-19de-452a-8887-82a74e275618",
   "metadata": {},
   "source": [
    "The \"leaky ReLU\" is a variant of the ReLU activation function that addresses the vanishing gradient problem by allowing a small, non-zero gradient when the input is negative. This prevents the neuron from becoming inactive, ensuring that it can continue to learn even for negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb479f3-282f-45e4-a416-e70a11c24b2c",
   "metadata": {},
   "source": [
    "#### Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683cf0f-e34e-4686-8cc9-194a061b4a7e",
   "metadata": {},
   "source": [
    "The softmax activation function is used in the output layer of a neural network to convert raw scores into probabilities. It ensures that the sum of the output probabilities is equal to 1, making it suitable for multi-class classification tasks. Softmax is commonly used in tasks such as image classification and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a795b7d-2954-4747-90ef-d6d59308004f",
   "metadata": {},
   "source": [
    "\n",
    "#### Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb12ecb-3795-4da7-8721-67589c61245a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
